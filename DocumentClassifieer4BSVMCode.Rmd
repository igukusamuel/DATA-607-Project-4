---
title: "DATA 607 - Project 4 - Document Classifier (Using SVM Model)"
author: "Project4 Team: Banu Boopalan, Samuel Kigamba, James Mundy, Alain T Kuiete"
output:
  pdf_document: default
  ioslides_presentation: default
  html_document:
    css:
    highlight: pygments
    theme: cerulean
    toc: true
    toc_float: true
    code_folding: hide
  slidy_presentation: default
---

# Project 4: PART 2 CODE (Submitted via this separate link. A separate link using RPUBS via Naive Bayes model will be submitted by our team)

Our Project Team 4 above (Banu Boopalan, Samuel Kigamba, James Mundy, Alain T Kuiete), we will submit 2 separate RPUB documents. The 2nd document link to RPUBS, we have performed data transformations, exploratory data analysis, visualizations using wordclouds, frequency plots on words, and performed SVM model and reported the Confusion Matrix results for the SVM model. We tried to plot the model using plot but we were not successful in representing a way to plot the model, The support vector are high range so we have to dive deeper into how to represent and plot the model through plot or Kernlab pacakge or Kernfit. Within the model we are able to create document term matrix and term document matrix, segment the train and test data and then run the model to report summary model. The SVM reported an accuracy for each of our teammates will be different as we are reading in our own files from the directory. The SVM reported higher accuracy than the Naive Bayes upon first review.

Collaboration via POWERPOINT, GITHUB, GOTO MEETING along with weekly meetings on Tuesday, Friday.

# Our Approach

We have utilized SVM model in this project4 code (Our first code that produced uses . Our approach for this project follows:

1. Load required Libraies
2. Get data from spamassassin website
3. Build a Build a Document Corpus
4. Plot Sentiment Analysis and Wordcloud of Corpus
4. Create Document-Term Matrix
5. Clean-up and Normalize Data
6. Create Training Set
7. Build/Train SVM 
8. Review Results  - Using Confusion Matrix Satistics, Use Radial and Linear type model



```{r echo=TRUE, warning=FALSE, message=FALSE}
#loading required Libraries
library(caret)
library(tidyverse)
library(tidyr)
library(dplyr)
library(stringr)
library(tidytext)
library(wordcloud)
library(broom)
library(tm)
library(e1071)
library(quanteda)
library(ggplot2)
```

## Get Data

#### The data for this project was obtained from: 

 https://spamassassin.apache.org/old/publiccorpus/
 
#### Ham and spam files were extracted and stored in a data folder on a local drive.


## Build a Corpus 

#### Next we build the corpus after completing some transforms: convert to plain doucment, remove stopwords, remove punctuation, remove numbers, remove whitespace, etc.

```{r echo=TRUE, warning=FALSE, message=FALSE}

create_corpus <- function(dir, label){
  corpus <- VCorpus(DirSource(dir)) %>%
    tm_map(PlainTextDocument)  %>%
    tm_map(content_transformer(tolower)) %>% # 
    tm_map(removeWords, stopwords("SMART")) %>% 
    tm_map(removePunctuation) %>% # 
    tm_map(removeNumbers) %>% # 
    tm_map(stripWhitespace) %>% # 
    tm_map(stemDocument) # 
  meta(corpus, "LABEL") <- label
  return(corpus)
}
corpus<- c(create_corpus("spam_2", "Spam"), create_corpus("easy_ham", "Ham"))


```

## Build a Document-Term Matrix

#### Now we use the corpus to construct a document term matrix and show wordcloud using Bing Lexicon

```{r echo=TRUE, warning=FALSE, message=FALSE}

dtm <- DocumentTermMatrix(corpus)
dtm

dtm_td <- tidy(dtm)
dtm_td

#slice sentiments of 1000 rows
dtm_sentiments <- slice(dtm_td , 1:5000) %>% inner_join(get_sentiments("bing"), by = c(term = "word"))
dtm_sentiments

#unnext tokens to look at words
slice_words <- tidy(corpus) %>%
  unnest_tokens(word, text) 
  slice_words <- slice(slice_words, 1:9000)
  
  library(broom)
models <- count(slice_words, word) %>% inner_join(get_sentiments("bing"), by = c(word = "word"))

str(models)

dtm_sentiments %>%
  count(document, sentiment, wt = count) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(sentiment)

dtm_sentiments %>%
  count(sentiment, term, wt = count) %>%
  filter(n <= 10) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment") + ggtitle("Bing Lexicon Sentiment Analysis for corpus")

dtm_sentiments %>%
  count(sentiment, term, wt = count) %>%
  top_n(50) %>%
  ungroup() %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip() 

#layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
#par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Wordcloud using Bing Lexicon for corpus")
wordcloud(words = dtm_sentiments$term, freq = dtm_sentiments$count, min.freq = 1,max.words=200, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))

```


### Reduce Sparseness and Normalize

#### We reduce sparness here by only keeping words that are found more than n times. We tried training the model with differnt values for n but found that 15 produced the best results . 


```{r echo=TRUE, warning=FALSE, message=FALSE }
#Only Keep Words found in at least 15 documents

min_docs <- 15
dtm <- removeSparseTerms(dtm, 1 - (min_docs / length(corpus)))

model_data <- as.matrix(dtm)
str(model_data)
words <- rowSums(model_data)
model_data <- model_data / words
model_data <- data.frame(model_data)
model_data <- cbind(meta(corpus), model_data) %>%
  mutate(LABEL = as.factor(LABEL))

```

## Create a Training Set

#### We now divide the data into training and test sets.  Seventy-five percent of the data was used for training.

```{r  echo=TRUE, warning=FALSE, message=FALSE}
set.seed(12345)
in_training_set <- createDataPartition(model_data$LABEL, p = 0.75,  list = FALSE)
training_data <- model_data[in_training_set, ]
testing_data <- model_data[-in_training_set, ]
#head(training_data,n=1)
nrow(testing_data)

```

## Build / Train SVM

#### We use the training data to build a SVM model that predicts if a message is spam or ham.

```{r echo=TRUE, warning=FALSE, message=FALSE}
#This outputs Radial kernal type
model <- svm(LABEL ~ ., data = training_data)
model

```

#### Use Kernal Linear type to see results 

```{r echo=TRUE, warning=FALSE, message=FALSE}

#This outputs linear kernal type
model1 <- svm(LABEL ~ ., data = training_data, kernel = "linear", scale = FALSE)
model1

```


## Review Results

#### Finally, we test our model to see how accurate it is. Predictions is Radial type and Predictions1 is Linear type.

```{r echo=TRUE, warning=FALSE, message=FALSE}
predictions <- testing_data %>%
  select(-LABEL) %>%
  predict(model, .)

predictions1 <- testing_data %>%
  select(-LABEL) %>%
  predict(model1, .)

#radial
table(Prediction = predictions ,Truth = testing_data$LABEL)
#linear
table(Prediction = predictions1 ,Truth = testing_data$LABEL)

```

#### The confusion matrix below indicates that with n = 15, only 8 emails were misclassified. This equates to approximately 99% accuracy. 

```{r echo=TRUE, warning=FALSE, message=FALSE}
#install.packages('kableExtra')
library(kableExtra)
table(predictions, testing_data$LABEL) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "responsive"))

#Radial Classification
confMatrix1 <- confusionMatrix(predictions, testing_data$LABEL)
confMatrix1 

#Linear Classification
confMatrix2 <- confusionMatrix(predictions1, testing_data$LABEL)
confMatrix2 

```

